{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction:\n",
    "---\n",
    "\n",
    "Money laundering is a multi-billion dollar issue. Detection of laundering is very difficult. Most automated algorithms have a high false positive rate: legitimate transactions incorrectly flagged as laundering. The converse is also a major problem -- false negatives, i.e. undetected laundering transactions. Naturally, criminals work hard to cover their tracks.\n",
    "\n",
    "Access to real financial transaction data is highly restricted, for both proprietary and privacy reasons. Even when access is possible, it is problematic to provide a correct tag (laundering or legitimate) to each transaction, as noted above. \n",
    "\n",
    "In this project we are using a synthetic transaction dataset from IBM that avoids these problems (ALTMAN et al. 2023).\n",
    "\n",
    "\n",
    "**To check the paper that originated this synthetic dataset, [click here!](https://arxiv.org/abs/2306.16424)**\n",
    "\n",
    "The data provided here is based on a virtual world inhabited by individuals, companies, and banks. Individuals interact with other individuals and companies. Likewise, companies interact with other companies and with individuals. These interactions can take many forms, e.g. purchase of consumer goods and services, purchase orders for industrial supplies, payment of salaries, repayment of loans, and more. These financial transactions are generally conducted via banks, i.e. the payer and receiver both have accounts, with accounts taking multiple forms from checking to credit cards to bitcoin.\n",
    "\n",
    "Some (small) fraction of the individuals and companies in the generator model engage in criminal behavior -- such as smuggling, illegal gambling, extortion, and more. Criminals obtain funds from these illicit activities, and then try to hide the source of these illicit funds via a series of financial transactions. Such financial transactions to hide illicit funds constitute laundering. Thus, the data available here is labelled and can be used for training and testing AML (Anti Money Laundering) models and for other purposes.\n",
    "\n",
    "The data generator that created the data here not only models illicit activity, but also tracks funds derived from illicit activity through arbitrarily many transactions -- thus creating the ability to label laundering transactions many steps removed from their illicit source. With this foundation, it is straightforward for the generator to label individual transactions as laundering or legitimate.\n",
    "\n",
    "Note that this IBM generator models the entire money laundering cycle:\n",
    "\n",
    "*   **Placement**: Sources like smuggling of illicit funds.\n",
    "*   **Layering**: Mixing the illicit funds into the financial system.\n",
    "*   **Integration**: Spending the illicit funds.\n",
    "\n",
    "\n",
    "As another capability possible only with synthetic data, note that a real bank or other institution typically has access to only a portion of the transactions involved in laundering: the transactions involving that bank. Transactions happening at other banks or between other banks are not seen. Thus, models built on real transactions from one institution can have only a limited view of the world.\n",
    "\n",
    "By contrast these synthetic transactions contain an entire financial ecosystem. Thus it may be possible to create laundering detection models that undertand the broad sweep of transactions across institutions, but apply those models to make inferences only about transactions at a particular bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Importing Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Verify if Data is Present\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "PATH = str(pathlib.Path.cwd())\n",
    "file_path = pathlib.Path(\"data/HI-Large_Trans.csv\")\n",
    "\n",
    "if not file_path.is_file():\n",
    "    with zipfile.ZipFile(\"./data.zip\", 'r') as zf:\n",
    "        zf.extractall(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analisys (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Reading the HI-Small_Trans file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"./data/HI-Small_Trans.csv\")\n",
    "\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Sampling a Portion of the Original DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df.sample(n=500000, random_state=42)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. About the Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Basic Statistic in the Numerical Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude='object').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_values_changer(col, zero, one):\n",
    "#     for i in range(col.shape[0]):\n",
    "#         if col.values[i] == zero:\n",
    "#             col.values[i] = 0\n",
    "#         elif col.values[i] == one:\n",
    "#             col.values[i] = 1\n",
    "#         else:\n",
    "#             col.values[i] = 2\n",
    "    \n",
    "#     return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the HI-Large_Trans.csv, 1000000 rows each time, isolating only 'Is Laundering' == 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# count = 1\n",
    "# for df in pd.read_csv('./data/HI-Large_Trans.csv', chunksize=1000000):\n",
    "#     df = df[df['Is Laundering'] == 1]\n",
    "    \n",
    "#     del df['Timestamp']\n",
    "#     dfs.append(df)\n",
    "    \n",
    "#     if count % 10 == 0:\n",
    "#         print(f\"{(count / 180)*100:.2f}% complete\")\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full_1 = pd.concat(dfs)\n",
    "# del dfs\n",
    "\n",
    "# ones_count = df_full_1.shape[0]\n",
    "# print(\"Number of rows with 'Is Laundering' == 1:\", ones_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the HI-Large_Trans.csv, 1000000 rows each time, isolating only 'Is Laundering' == 0, until it becames 1:1 ratio with 'Is Laundering' == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# current = 0\n",
    "# for df in pd.read_csv('./data/HI-Large_Trans.csv', chunksize=15000):\n",
    "#     df = df[df['Is Laundering'] == 0]\n",
    "#     current += df.shape[0]\n",
    "    \n",
    "#     del df['Timestamp']\n",
    "#     dfs.append(df)\n",
    "\n",
    "#     if current >= ones_count:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full_0 = pd.concat(dfs)\n",
    "# df_full = pd.concat([df_full_0, df_full_1])\n",
    "# del dfs\n",
    "\n",
    "# zeros_count = df_full_0.shape[0]\n",
    "# print(\"Number of rows with 'Is Laundering' == 0:\", zeros_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Unique values for feature:\")\n",
    "# {feature:len(df_full[feature].unique()) for feature in df_full.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"./data/HI-Small_Trans.csv\")\n",
    "\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns representing paid and received amount of each transcation, wondering if it is necessary to split the amount into two columns when they shared the same value, unless there are transcation fee/transcation between different currency. Let's find out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amount Received equals to Amount Paid:')\n",
    "print(full_df['Amount Received'].equals(full_df['Amount Paid']))\n",
    "print('Receiving Currency equals to Payment Currency:')\n",
    "print(full_df['Receiving Currency'].equals(full_df['Payment Currency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_equal1 = full_df.loc[~(full_df['Amount Received'] == full_df['Amount Paid'])]\n",
    "not_equal2 = full_df.loc[~(full_df['Receiving Currency'] == full_df['Payment Currency'])]\n",
    "print(\"Transactions with different amount received and paid\")\n",
    "display(not_equal1.head())\n",
    "print('---------------------------------------------------------------------------')\n",
    "print(\"Transactions with different currency received and paid\")\n",
    "display(not_equal2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the values of `Receiving Currency` and `Payment Currency` match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(full_df['Receiving Currency'].unique()))\n",
    "print(sorted(full_df['Payment Currency'].unique()))\n",
    "print(\"Equal?\",sorted(full_df['Receiving Currency'].unique()) == sorted(full_df['Payment Currency'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data preprocessing, we perform below transformation:  \n",
    "1. Transform the Timestamp with min max normalization.  \n",
    "2. Create unique ID for each account by adding bank code with account number.  \n",
    "3. Create receiving_df with the information of receiving accounts, received amount and currency\n",
    "4. Create paying_df with the information of payer accounts, paid amount and currency\n",
    "5. Create a list of currency used among all transactions\n",
    "6. Label the 'Payment Format', 'Payment Currency', 'Receiving Currency' by classes with sklearn OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Approach, now using GNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Step:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step, we will:  \n",
    "1. Transform the Timestamp from: `datetime` to the difference from start time and current time **in seconds**.\n",
    "\n",
    "\n",
    "2. Create a dictionary for all `Received Currency` and `Payment Currency`.\n",
    "\n",
    "\n",
    "3. Create a dictionary for `Payment Format`.\n",
    "\n",
    "\n",
    "4. Create a dictionary for with the concat of `From Bank` and `Account`.\n",
    "\n",
    "\n",
    "5. Create a column (feature `from_id`) with the unique ids from the dictionary of the previous Step.\n",
    "\n",
    "\n",
    "6. Create a dictionary for with the concat of `To Bank` and `Account.1`.\n",
    "\n",
    "\n",
    "7. Create a column (feature `to_id`) with the unique ids from the dictionary of the previous Step.\n",
    "\n",
    "\n",
    "8. Drop unused and temporary features.\n",
    "\n",
    "\n",
    "9. Reindex and sort data based on `Timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_dict_val(name, collection):\n",
    "    if name in collection:\n",
    "        val = collection[name]\n",
    "    else:\n",
    "        val = len(collection)\n",
    "        collection[name] = val\n",
    "    return val\n",
    "\n",
    "def format_timestamp(timestamp):\n",
    "    firstTs = -1\n",
    "    timestamps = []\n",
    "    for i in timestamp:\n",
    "        dt_ts = datetime.strptime(i, '%Y/%m/%d %H:%M')\n",
    "        ts = dt_ts.timestamp()\n",
    "        if firstTs == -1:\n",
    "            day = dt_ts.day\n",
    "            month = dt_ts.month\n",
    "            year = dt_ts.year\n",
    "            startTime = datetime(year, month, day)\n",
    "            firstTs = startTime.timestamp() - 10\n",
    "        ts = ts - firstTs\n",
    "        timestamps.append(ts)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "df_edges = pd.read_csv(\"./data/HI-Small_Trans.csv\")\n",
    "\n",
    "# Dictionaries to store keys (name of the features) and values (unique ids)\n",
    "currency = dict()\n",
    "payment_format = dict()\n",
    "fromAccIdStr = dict()\n",
    "toAccIdStr = dict()\n",
    "\n",
    "df_edges[\"Timestamp\"] = format_timestamp(df_edges[\"Timestamp\"])\n",
    "df_edges[\"Received Currency\"] = df_edges['Receiving Currency'].apply(lambda x: get_dict_val(x, currency))\n",
    "df_edges[\"Sent Currency\"] = df_edges['Payment Currency'].apply(lambda x: get_dict_val(x, currency))\n",
    "df_edges[\"Payment Format\"] = df_edges['Payment Format'].apply(lambda x: get_dict_val(x, payment_format))\n",
    "df_edges[\"temp_1\"] = df_edges[\"From Bank\"].astype(str) + df_edges[\"Account\"].astype(str)\n",
    "df_edges[\"from_id\"] = df_edges[\"temp_1\"].apply(lambda x: get_dict_val(x, fromAccIdStr))\n",
    "df_edges[\"temp_2\"] = df_edges[\"To Bank\"].astype(str) + df_edges[\"Account.1\"].astype(str)\n",
    "df_edges[\"to_id\"] = df_edges[\"temp_2\"].apply(lambda x: get_dict_val(x, toAccIdStr))\n",
    "\n",
    "df_edges.reset_index(drop=True, inplace=True)\n",
    "df_edges[\"EdgeID\"] = df_edges.index\n",
    "\n",
    "df_edges.rename(columns={\"Amount Paid\":\"Amount Sent\"}, inplace=True)\n",
    "\n",
    "df_edges.drop(columns=[\"temp_1\", \"temp_2\", \"From Bank\", \"Account\",\n",
    "                       \"To Bank\", \"Account.1\", \"Receiving Currency\",\n",
    "                       \"Payment Currency\"], inplace=True)\n",
    "\n",
    "df_edges = df_edges.reindex(columns=[\"EdgeID\",\"from_id\",\"to_id\",\"Timestamp\",\n",
    "                                     \"Amount Sent\",\"Sent Currency\",\"Amount Received\",\n",
    "                                     \"Received Currency\",\"Payment Format\",\"Is Laundering\"])\n",
    "\n",
    "df_edges[\"Timestamp\"] = df_edges[\"Timestamp\"] - df_edges[\"Timestamp\"].min()\n",
    "df_edges = df_edges.sort_values(by=\"Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, we will:  \n",
    "1. Create a DataFrame `df_nodes` with all nodes (from node 0 to node $N$).\n",
    "\n",
    "\n",
    "2. Convert the `Timestamp` feature to `torch.Tensor`.\n",
    "\n",
    "\n",
    "3. Convert the label `Is Laundering` to `torch.LongTensor`\n",
    "\n",
    "\n",
    "4. Create the `torch.tensor` $X$ with the node_features\n",
    "\n",
    "\n",
    "5. Create the `edge_index` matrix ($2 * $Num$) where $Num$ is the number of edges\n",
    "\n",
    "\n",
    "6. Create the `edge_attr` matrix ($Num$ * $Feature$) where $Num$ is the number of edges and $Feature$ is the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get the number of the highest id (it will then become the number of nodes in our graph)\n",
    "max_n_id = df_edges.loc[:, ['from_id', 'to_id']].to_numpy().max() + 1 # -> 496999 nodes\n",
    "\n",
    "# Create a dataframe with all nodes, with a placeholder (1.0) for the features\n",
    "df_nodes = pd.DataFrame({'NodeID': np.arange(max_n_id), 'Feature': np.ones(max_n_id)})\n",
    "\n",
    "# Convert the `Timestamp` to a torch.Tensor\n",
    "timestamps = torch.Tensor(df_edges['Timestamp'].to_numpy())\n",
    "\n",
    "# Convert the label `Is Laundering` to a torch.LongTensor\n",
    "y = torch.LongTensor(df_edges['Is Laundering'].to_numpy()) # -> 5.078.345 rows (vector)\n",
    "\n",
    "edge_features = ['Timestamp', 'Amount Received', 'Received Currency', 'Payment Format']\n",
    "node_features = ['Feature']\n",
    "\n",
    "X = torch.tensor(df_nodes.loc[:, node_features].to_numpy()).float() # -> 496.999 rows x 1 column (matrix)\n",
    "\n",
    "edge_index = torch.LongTensor(df_edges.loc[:, ['from_id', 'to_id']].to_numpy().T) # -> (2 x 5078345) matrix\n",
    "edge_attr = torch.tensor(df_edges.loc[:, edge_features].to_numpy()).float() # -> (5078345 x 4) matrix\n",
    "\n",
    "n_days = int(timestamps.max() / (3600 * 24) + 1)\n",
    "n_samples = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, we will:  \n",
    "1. Create 4 lists:\n",
    "    *   `daily_irs` -> mean of illicit transactions for each day.\n",
    "    *   `weighted_daily_irs` -> daily_irs weighted by the number of samples.\n",
    "    *   `daily_inds` -> indices for all transactions within each day.\n",
    "    *   `daily_trans` -> number of transactions for each day.\n",
    "\n",
    "\n",
    "2. Get the best train/val/test split based on the given proportions `[0.6, 0.2, 0.2]`.\n",
    "\n",
    "\n",
    "3. Define the days that will be part of:\n",
    "   *    `Train`: days before `i`.\n",
    "   *    `Validation`: days between `i` e `j`.\n",
    "   *    `Test`: days after `j`.\n",
    "\n",
    "\n",
    "4. Concat all indices from the given split into a `torch.tensor`\n",
    "    *    `tr_inds`: indices for training.\n",
    "    *    `val_inds`: indices for validation.\n",
    "    *    `te_inds`: indices for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#data splitting\n",
    "daily_irs, weighted_daily_irs, daily_inds, daily_trans = [], [], [], [] #irs = illicit\n",
    "\n",
    "for day in range(n_days):\n",
    "        l = day * 24 * 3600\n",
    "        r = (day + 1) * 24 * 3600\n",
    "        day_inds = torch.where((timestamps >= l) & (timestamps < r))[0]\n",
    "        daily_irs.append(y[day_inds].float().mean())\n",
    "        weighted_daily_irs.append(y[day_inds].float().mean() * day_inds.shape[0] / n_samples)\n",
    "        daily_inds.append(day_inds)\n",
    "        daily_trans.append(day_inds.shape[0])\n",
    "\n",
    "# Recommended split_percentages for train, validation and test. \n",
    "split_per = [0.6, 0.2, 0.2]\n",
    "daily_totals = np.array(daily_trans)\n",
    "d_ts = daily_totals\n",
    "I = list(range(len(d_ts)))\n",
    "split_scores = dict()\n",
    "\n",
    "# Iterates over all days combination ranges and stores the score at split_scores\n",
    "for i,j in itertools.combinations(I, 2):\n",
    "    if j >= i:\n",
    "        split_totals = [d_ts[:i].sum(), d_ts[i:j].sum(), d_ts[j:].sum()]\n",
    "        split_totals_sum = np.sum(split_totals)\n",
    "        split_props = [v/split_totals_sum for v in split_totals] # proportion of each split compared to the total transactions\n",
    "        split_error = [abs(v-t)/t for v,t in zip(split_props, split_per)] \n",
    "        score = max(split_error) #- (split_totals_sum/total) + 1\n",
    "        split_scores[(i,j)] = score\n",
    "    else:\n",
    "        continue\n",
    "i,j = min(split_scores, key=split_scores.get) # get the best i,j from split_scores\n",
    "\n",
    "# split contains a list for each split (train, validation and test) and each list contains the days that are part of the respective split\n",
    "split = [list(range(i)), list(range(i, j)), list(range(j, len(daily_totals)))]\n",
    "\n",
    "# seperate the transactions based on their indices in the timestamp array\n",
    "split_inds = {k: [] for k in range(3)}\n",
    "for i in range(3):\n",
    "    for day in split[i]:\n",
    "        split_inds[i].append(daily_inds[day]) #split_inds contains a list for each split (tr,val,te) which contains the indices of each day seperately\n",
    "\n",
    "tr_inds = torch.cat(split_inds[0])\n",
    "val_inds = torch.cat(split_inds[1])\n",
    "te_inds = torch.cat(split_inds[2])\n",
    "\n",
    "tr_x, val_x, te_x = X, X, X # sets the placeholder (ones) to the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total train samples: {tr_inds.shape[0] / y.shape[0] * 100 :.2f}% || IR: \"\n",
    "        f\"{y[tr_inds].float().mean() * 100 :.2f}% || Train days: {split[0]}\")\n",
    "print(f\"Total val samples: {val_inds.shape[0] / y.shape[0] * 100 :.2f}% || IR: \"\n",
    "    f\"{y[val_inds].float().mean() * 100:.2f}% || Val days: {split[1]}\")\n",
    "print(f\"Total test samples: {te_inds.shape[0] / y.shape[0] * 100 :.2f}% || IR: \"\n",
    "    f\"{y[te_inds].float().mean() * 100:.2f}% || Test days: {split[2]}\")\n",
    "\n",
    "# IR stants for Illicit Ratio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tr = tr_inds.numpy() # Edge train array\n",
    "e_val = np.concatenate([tr_inds, val_inds]) # Edge validation (train + val) array\n",
    "\n",
    "# Train\n",
    "tr_edge_index, tr_edge_attr, tr_y, tr_edge_times = edge_index[:,e_tr],  edge_attr[e_tr],  y[e_tr],  timestamps[e_tr]\n",
    "\n",
    "# Validation (tr + val)\n",
    "val_edge_index, val_edge_attr, val_y, val_edge_times = edge_index[:,e_val], edge_attr[e_val], y[e_val], timestamps[e_val]\n",
    "\n",
    "# Test (tr + val + te)\n",
    "te_edge_index, te_edge_attr, te_y, te_edge_times = edge_index, edge_attr, y, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_util\n",
    "\n",
    "tr_data = data_util.GraphData(x=tr_x, y=tr_y, edge_index=tr_edge_index, edge_attr=tr_edge_attr, timestamps=tr_edge_times)\n",
    "val_data = data_util.GraphData(x=val_x, y=val_y, edge_index=val_edge_index, edge_attr=val_edge_attr, timestamps=val_edge_times)\n",
    "te_data = data_util.GraphData(x=te_x, y=te_y, edge_index=te_edge_index, edge_attr=te_edge_attr, timestamps=te_edge_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('args.json', 'r') as config_file:\n",
    "        data_config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GINEConv, BatchNorm, Linear, GATConv, PNAConv, RGCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class GINe(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_gnn_layers, n_classes=2, \n",
    "                n_hidden=100, edge_updates=False, residual=True, \n",
    "                edge_dim=None, dropout=0.0, final_dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.edge_updates = edge_updates\n",
    "        self.final_dropout = final_dropout\n",
    "\n",
    "        self.node_emb = nn.Linear(num_features, n_hidden)\n",
    "        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.emlps = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for _ in range(self.num_gnn_layers):\n",
    "            conv = GINEConv(nn.Sequential(\n",
    "                nn.Linear(self.n_hidden, self.n_hidden), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(self.n_hidden, self.n_hidden)\n",
    "                ), edge_dim=self.n_hidden)\n",
    "            if self.edge_updates: self.emlps.append(nn.Sequential(\n",
    "                nn.Linear(3 * self.n_hidden, self.n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            ))\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(n_hidden))\n",
    "\n",
    "        self.mlp = nn.Sequential(Linear(n_hidden*3, 50), nn.ReLU(), nn.Dropout(self.final_dropout),Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),\n",
    "                              Linear(25, n_classes))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        src, dst = edge_index\n",
    "\n",
    "        x = self.node_emb(x)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x = (x + F.relu(self.batch_norms[i](self.convs[i](x, edge_index, edge_attr)))) / 2\n",
    "            if self.edge_updates: \n",
    "                edge_attr = edge_attr + self.emlps[i](torch.cat([x[src], x[dst], edge_attr], dim=-1)) / 2\n",
    "\n",
    "        x = x[edge_index.T].reshape(-1, 2 * self.n_hidden).relu()\n",
    "        x = torch.cat((x, edge_attr.view(-1, edge_attr.shape[1])), 1)\n",
    "        out = x\n",
    "        \n",
    "        return self.mlp(out)\n",
    "    \n",
    "class GATe(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_gnn_layers, n_classes=2, n_hidden=100, n_heads=4, edge_updates=False, edge_dim=None, dropout=0.0, final_dropout=0.5):\n",
    "        super().__init__()\n",
    "        # GAT specific code\n",
    "        tmp_out = n_hidden // n_heads\n",
    "        n_hidden = tmp_out * n_heads\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_heads = n_heads\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.edge_updates = edge_updates\n",
    "        self.dropout = dropout\n",
    "        self.final_dropout = final_dropout\n",
    "        \n",
    "        self.node_emb = nn.Linear(num_features, n_hidden)\n",
    "        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.emlps = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_gnn_layers):\n",
    "            conv = GATConv(self.n_hidden, tmp_out, self.n_heads, concat = True, dropout = self.dropout, add_self_loops = True, edge_dim=self.n_hidden)\n",
    "            if self.edge_updates: self.emlps.append(nn.Sequential(nn.Linear(3 * self.n_hidden, self.n_hidden),nn.ReLU(),nn.Linear(self.n_hidden, self.n_hidden),))\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(n_hidden))\n",
    "                \n",
    "        self.mlp = nn.Sequential(Linear(n_hidden*3, 50), nn.ReLU(), nn.Dropout(self.final_dropout),Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),Linear(25, n_classes))\n",
    "            \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        src, dst = edge_index\n",
    "        \n",
    "        x = self.node_emb(x)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "        \n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x = (x + F.relu(self.batch_norms[i](self.convs[i](x, edge_index, edge_attr)))) / 2\n",
    "            if self.edge_updates:\n",
    "                edge_attr = edge_attr + self.emlps[i](torch.cat([x[src], x[dst], edge_attr], dim=-1)) / 2\n",
    "                    \n",
    "        logging.debug(f\"x.shape = {x.shape}, x[edge_index.T].shape = {x[edge_index.T].shape}\")\n",
    "        x = x[edge_index.T].reshape(-1, 2 * self.n_hidden).relu()\n",
    "        logging.debug(f\"x.shape = {x.shape}\")\n",
    "        x = torch.cat((x, edge_attr.view(-1, edge_attr.shape[1])), 1)\n",
    "        logging.debug(f\"x.shape = {x.shape}\")\n",
    "        out = x\n",
    "\n",
    "        return self.mlp(out)\n",
    "    \n",
    "class PNA(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_gnn_layers, n_classes=2, \n",
    "                n_hidden=100, edge_updates=True,\n",
    "                edge_dim=None, dropout=0.0, final_dropout=0.5, deg=None):\n",
    "        super().__init__()\n",
    "        n_hidden = int((n_hidden // 5) * 5)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.edge_updates = edge_updates\n",
    "        self.final_dropout = final_dropout\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.node_emb = nn.Linear(num_features, n_hidden)\n",
    "        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.emlps = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for _ in range(self.num_gnn_layers):\n",
    "            conv = PNAConv(in_channels=n_hidden, out_channels=n_hidden,\n",
    "                           aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                           edge_dim=n_hidden, towers=5, pre_layers=1, post_layers=1,\n",
    "                           divide_input=False)\n",
    "            if self.edge_updates: self.emlps.append(nn.Sequential(\n",
    "                nn.Linear(3 * self.n_hidden, self.n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            ))\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(n_hidden))\n",
    "\n",
    "        self.mlp = nn.Sequential(Linear(n_hidden*3, 50), nn.ReLU(), nn.Dropout(self.final_dropout),Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),\n",
    "                              Linear(25, n_classes))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        src, dst = edge_index\n",
    "\n",
    "        x = self.node_emb(x)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x = (x + F.relu(self.batch_norms[i](self.convs[i](x, edge_index, edge_attr)))) / 2\n",
    "            if self.edge_updates: \n",
    "                edge_attr = edge_attr + self.emlps[i](torch.cat([x[src], x[dst], edge_attr], dim=-1)) / 2\n",
    "\n",
    "        logging.debug(f\"x.shape = {x.shape}, x[edge_index.T].shape = {x[edge_index.T].shape}\")\n",
    "        x = x[edge_index.T].reshape(-1, 2 * self.n_hidden).relu()\n",
    "        logging.debug(f\"x.shape = {x.shape}\")\n",
    "        x = torch.cat((x, edge_attr.view(-1, edge_attr.shape[1])), 1)\n",
    "        logging.debug(f\"x.shape = {x.shape}\")\n",
    "        out = x\n",
    "        return self.mlp(out)\n",
    "    \n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_features, edge_dim, num_relations, num_gnn_layers, n_classes=2, \n",
    "                n_hidden=100, edge_update=False,\n",
    "                residual=True,\n",
    "                dropout=0.0, final_dropout=0.5, n_bases=-1):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.residual = residual\n",
    "        self.dropout = dropout\n",
    "        self.final_dropout = final_dropout\n",
    "        self.n_classes = n_classes\n",
    "        self.edge_update = edge_update\n",
    "        self.num_relations = num_relations\n",
    "        self.n_bases = n_bases\n",
    "\n",
    "        self.node_emb = nn.Linear(num_features, n_hidden)\n",
    "        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.mlp = nn.ModuleList()\n",
    "\n",
    "        if self.edge_update:\n",
    "            self.emlps = nn.ModuleList()\n",
    "            self.emlps.append(nn.Sequential(\n",
    "                nn.Linear(3 * self.n_hidden, self.n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            ))\n",
    "        \n",
    "        for _ in range(self.num_gnn_layers):\n",
    "            conv = RGCNConv(self.n_hidden, self.n_hidden, num_relations, num_bases=self.n_bases)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(nn.BatchNorm1d(self.n_hidden))\n",
    "\n",
    "            if self.edge_update:\n",
    "                self.emlps.append(nn.Sequential(\n",
    "                    nn.Linear(3 * self.n_hidden, self.n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                ))\n",
    "\n",
    "        self.mlp = nn.Sequential(Linear(n_hidden*3, 50), nn.ReLU(), nn.Dropout(self.final_dropout), Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),\n",
    "                              Linear(25, n_classes))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()\n",
    "            elif isinstance(m, RGCNConv):\n",
    "                m.reset_parameters()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_type = edge_attr[:, -1].long()\n",
    "        #edge_attr = edge_attr[:, :-1]\n",
    "        src, dst = edge_index\n",
    "\n",
    "        x = self.node_emb(x)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x =  (x + F.relu(self.bns[i](self.convs[i](x, edge_index, edge_type)))) / 2\n",
    "            if self.edge_update:\n",
    "                edge_attr = (edge_attr + F.relu(self.emlps[i](torch.cat([x[src], x[dst], edge_attr], dim=-1)))) / 2\n",
    "        \n",
    "        x = x[edge_index.T].reshape(-1, 2 * self.n_hidden).relu()\n",
    "        x = torch.cat((x, edge_attr.view(-1, edge_attr.shape[1])), 1)\n",
    "        x = self.mlp(x)\n",
    "        out = x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from typing import Union\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "\n",
    "class AddEgoIds(BaseTransform):\n",
    "    r\"\"\"Add IDs to the centre nodes of the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data: Union[Data, HeteroData]):\n",
    "        x = data.x if not isinstance(data, HeteroData) else data['node'].x\n",
    "        device = x.device\n",
    "        ids = torch.zeros((x.shape[0], 1), device=device)\n",
    "        if not isinstance(data, HeteroData):\n",
    "            nodes = torch.unique(data.edge_label_index.view(-1)).to(device)\n",
    "        else:\n",
    "            nodes = torch.unique(data['node', 'to', 'node'].edge_label_index.view(-1)).to(device)\n",
    "        ids[nodes] = 1\n",
    "        if not isinstance(data, HeteroData):\n",
    "            data.x = torch.cat([x, ids], dim=1)\n",
    "        else: \n",
    "            data['node'].x = torch.cat([x, ids], dim=1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "def extract_param(parameter_name: str, args) -> float:\n",
    "    \"\"\"\n",
    "    Extract the value of the specified parameter for the given model.\n",
    "    \n",
    "    Args:\n",
    "    - parameter_name (str): Name of the parameter (e.g., \"lr\").\n",
    "    - args (argparser): Arguments given to this specific run.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Value of the specified parameter.\n",
    "    \"\"\"\n",
    "    file_path = './model_settings.json'\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data.get(args[\"model\"], {}).get(\"params\", {}).get(parameter_name, None)\n",
    "\n",
    "def add_arange_ids(data_list):\n",
    "    '''\n",
    "    Add the index as an id to the edge features to find seed edges in training, validation and testing.\n",
    "\n",
    "    Args:\n",
    "    - data_list (str): List of tr_data, val_data and te_data.\n",
    "    '''\n",
    "    for data in data_list:\n",
    "        if isinstance(data, HeteroData):\n",
    "            data['node', 'to', 'node'].edge_attr = torch.cat([torch.arange(data['node', 'to', 'node'].edge_attr.shape[0]).view(-1, 1), data['node', 'to', 'node'].edge_attr], dim=1)\n",
    "            offset = data['node', 'to', 'node'].edge_attr.shape[0]\n",
    "            data['node', 'rev_to', 'node'].edge_attr = torch.cat([torch.arange(offset, data['node', 'rev_to', 'node'].edge_attr.shape[0] + offset).view(-1, 1), data['node', 'rev_to', 'node'].edge_attr], dim=1)\n",
    "        else:\n",
    "            data.edge_attr = torch.cat([torch.arange(data.edge_attr.shape[0]).view(-1, 1), data.edge_attr], dim=1)\n",
    "\n",
    "def get_loaders(tr_data, val_data, te_data, tr_inds, val_inds, te_inds, transform, args):\n",
    "    if isinstance(tr_data, HeteroData):\n",
    "        tr_edge_label_index = tr_data['node', 'to', 'node'].edge_index\n",
    "        tr_edge_label = tr_data['node', 'to', 'node'].y\n",
    "\n",
    "\n",
    "        tr_loader =  LinkNeighborLoader(tr_data, num_neighbors=args[\"num_neighs\"], \n",
    "                                    edge_label_index=(('node', 'to', 'node'), tr_edge_label_index), \n",
    "                                    edge_label=tr_edge_label, batch_size=args[\"batch_size\"], shuffle=True, transform=transform)\n",
    "        \n",
    "        val_edge_label_index = val_data['node', 'to', 'node'].edge_index[:,val_inds]\n",
    "        val_edge_label = val_data['node', 'to', 'node'].y[val_inds]\n",
    "\n",
    "\n",
    "        val_loader =  LinkNeighborLoader(val_data, num_neighbors=args[\"num_neighs\"], \n",
    "                                    edge_label_index=(('node', 'to', 'node'), val_edge_label_index), \n",
    "                                    edge_label=val_edge_label, batch_size=args[\"batch_size\"], shuffle=False, transform=transform)\n",
    "        \n",
    "        te_edge_label_index = te_data['node', 'to', 'node'].edge_index[:,te_inds]\n",
    "        te_edge_label = te_data['node', 'to', 'node'].y[te_inds]\n",
    "\n",
    "\n",
    "        te_loader =  LinkNeighborLoader(te_data, num_neighbors=args[\"num_neighs\"], \n",
    "                                    edge_label_index=(('node', 'to', 'node'), te_edge_label_index), \n",
    "                                    edge_label=te_edge_label, batch_size=args[\"batch_size\"], shuffle=False, transform=transform)\n",
    "    else:\n",
    "        tr_loader =  LinkNeighborLoader(tr_data, num_neighbors=args[\"num_neighs\"], batch_size=args[\"batch_size\"], shuffle=True, transform=transform)\n",
    "        val_loader = LinkNeighborLoader(val_data,num_neighbors=args[\"num_neighs\"], edge_label_index=val_data.edge_index[:, val_inds],\n",
    "                                        edge_label=val_data.y[val_inds], batch_size=args[\"batch_size\"], shuffle=False, transform=transform)\n",
    "        te_loader =  LinkNeighborLoader(te_data,num_neighbors=args[\"num_neighs\"], edge_label_index=te_data.edge_index[:, te_inds],\n",
    "                                edge_label=te_data.y[te_inds], batch_size=args[\"batch_size\"], shuffle=False, transform=transform)\n",
    "        \n",
    "    return tr_loader, val_loader, te_loader\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_homo(loader, inds, model, data, device, args):\n",
    "    '''Evaluates the model performane for homogenous graph data.'''\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for batch in tqdm.tqdm(loader, disable=not args[\"tqdm\"]):\n",
    "        #select the seed edges from which the batch was created\n",
    "        inds = inds.detach().cpu()\n",
    "        batch_edge_inds = inds[batch.input_id.detach().cpu()]\n",
    "        batch_edge_ids = loader.data.edge_attr.detach().cpu()[batch_edge_inds, 0]\n",
    "        mask = torch.isin(batch.edge_attr[:, 0].detach().cpu(), batch_edge_ids)\n",
    "\n",
    "        #add the seed edges that have not been sampled to the batch\n",
    "        missing = ~torch.isin(batch_edge_ids, batch.edge_attr[:, 0].detach().cpu())\n",
    "\n",
    "        if missing.sum() != 0 and (args[\"data\"] == 'Small_J' or args[\"data\"] == 'Small_Q'):\n",
    "            missing_ids = batch_edge_ids[missing].int()\n",
    "            n_ids = batch.n_id\n",
    "            add_edge_index = data.edge_index[:, missing_ids].detach().clone()\n",
    "            node_mapping = {value.item(): idx for idx, value in enumerate(n_ids)}\n",
    "            add_edge_index = torch.tensor([[node_mapping[val.item()] for val in row] for row in add_edge_index])\n",
    "            add_edge_attr = data.edge_attr[missing_ids, :].detach().clone()\n",
    "            add_y = data.y[missing_ids].detach().clone()\n",
    "        \n",
    "            batch.edge_index = torch.cat((batch.edge_index, add_edge_index), 1)\n",
    "            batch.edge_attr = torch.cat((batch.edge_attr, add_edge_attr), 0)\n",
    "            batch.y = torch.cat((batch.y, add_y), 0)\n",
    "\n",
    "            mask = torch.cat((mask, torch.ones(add_y.shape[0], dtype=torch.bool)))\n",
    "\n",
    "        #remove the unique edge id from the edge features, as it's no longer needed\n",
    "        batch.edge_attr = batch.edge_attr[:, 1:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            out = out[mask]\n",
    "            pred = out.argmax(dim=-1)\n",
    "            preds.append(pred)\n",
    "            ground_truths.append(batch.y[mask])\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    f1 = f1_score(ground_truth, pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_hetero(loader, inds, model, data, device, args):\n",
    "    '''Evaluates the model performane for heterogenous graph data.'''\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for batch in tqdm.tqdm(loader, disable=not args[\"tqdm\"]):\n",
    "        #select the seed edges from which the batch was created\n",
    "        inds = inds.detach().cpu()\n",
    "        batch_edge_inds = inds[batch['node', 'to', 'node'].input_id.detach().cpu()]\n",
    "        batch_edge_ids = loader.data['node', 'to', 'node'].edge_attr.detach().cpu()[batch_edge_inds, 0]\n",
    "        mask = torch.isin(batch['node', 'to', 'node'].edge_attr[:, 0].detach().cpu(), batch_edge_ids)\n",
    "\n",
    "        #add the seed edges that have not been sampled to the batch\n",
    "        missing = ~torch.isin(batch_edge_ids, batch['node', 'to', 'node'].edge_attr[:, 0].detach().cpu())\n",
    "\n",
    "        if missing.sum() != 0 and (args[\"data\"] == 'Small_J' or args[\"data\"] == 'Small_Q'):\n",
    "            missing_ids = batch_edge_ids[missing].int()\n",
    "            n_ids = batch['node'].n_id\n",
    "            add_edge_index = data['node', 'to', 'node'].edge_index[:, missing_ids].detach().clone()\n",
    "            node_mapping = {value.item(): idx for idx, value in enumerate(n_ids)}\n",
    "            add_edge_index = torch.tensor([[node_mapping[val.item()] for val in row] for row in add_edge_index])\n",
    "            add_edge_attr = data['node', 'to', 'node'].edge_attr[missing_ids, :].detach().clone()\n",
    "            add_y = data['node', 'to', 'node'].y[missing_ids].detach().clone()\n",
    "        \n",
    "            batch['node', 'to', 'node'].edge_index = torch.cat((batch['node', 'to', 'node'].edge_index, add_edge_index), 1)\n",
    "            batch['node', 'to', 'node'].edge_attr = torch.cat((batch['node', 'to', 'node'].edge_attr, add_edge_attr), 0)\n",
    "            batch['node', 'to', 'node'].y = torch.cat((batch['node', 'to', 'node'].y, add_y), 0)\n",
    "\n",
    "            mask = torch.cat((mask, torch.ones(add_y.shape[0], dtype=torch.bool)))\n",
    "\n",
    "        #remove the unique edge id from the edge features, as it's no longer needed\n",
    "        batch['node', 'to', 'node'].edge_attr = batch['node', 'to', 'node'].edge_attr[:, 1:]\n",
    "        batch['node', 'rev_to', 'node'].edge_attr = batch['node', 'rev_to', 'node'].edge_attr[:, 1:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch.to(device)\n",
    "            out = model(batch.x_dict, batch.edge_index_dict, batch.edge_attr_dict)\n",
    "            out = out[('node', 'to', 'node')]\n",
    "            out = out[mask]\n",
    "            pred = out.argmax(dim=-1)\n",
    "            preds.append(pred)\n",
    "            ground_truths.append(batch['node', 'to', 'node'].y[mask])\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    f1 = f1_score(ground_truth, pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def save_model(model, optimizer, epoch, args, data_config):\n",
    "    # Save the model in a dictionary\n",
    "    torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, f'{data_config[\"paths\"][\"model_to_save\"]}/checkpoint_{args[\"unique_name\"]}{\"\" if not args[\"finetune\"] else \"_finetuned\"}.tar')\n",
    "    \n",
    "def load_model(model, device, args, config, data_config):\n",
    "    checkpoint = torch.load(f'{data_config[\"paths\"][\"model_to_load\"]}/checkpoint_{args[\"unique_name\"]}.tar')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
